{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Speech Recognition with Tensorflow</center>\n",
    "\n",
    "### <center>Course Project Report<center>\n",
    "###    <center>Submitted in partial fulfilment of the requirements for the<center>\n",
    "### <center>degree of<center>\n",
    "###    <center>Master of Technology in<center>\n",
    "### <center>Computer Science and Engineering<center>\n",
    "### <center>Under the guidance of<center>\n",
    "### <center>Dr. Annappa B<center>\n",
    "![Dataset](nitklogo.png)\n",
    "<div style=\"text-align: right\">\n",
    "\n",
    "<b>172CS007 &nbsp; Tanmay Badhe\n",
    "<br>\n",
    "<b>172CS009 &nbsp; &nbsp;&nbsp;&nbsp;&nbsp; Debojyoti M\n",
    "<br>\n",
    "<b>172CS015 &nbsp; &nbsp; &nbsp;&nbsp;&nbsp; Kemanth PJ\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Contents</center>\n",
    "\n",
    "### Topic  ..................................................................................  Page no.\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "### 1. Abstract ..................................................................................... 3\n",
    "<br>\n",
    "### 2. Introduction ............................................................................... 3\n",
    "<br>\n",
    "### 3. Speech Recognition with Tensorflow ..................................... 4\n",
    "<br>\n",
    "### 4. Implementation of Speech Recognizer in Tensorflow ........... 6\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Speech Recognition with Tensorflow</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">\n",
    "TensorFlow™ is an open source software library for high performance\n",
    "numerical computation. Its flexible architecture allows easy deployment of\n",
    "computation across a variety of platforms (CPUs, GPUs, TPUs), and from\n",
    "desktops to clusters of servers to mobile and edge devices. Originally\n",
    "developed by researchers and engineers from the Google Brain team within\n",
    "Google’s AI organization, it comes with strong support for machine learning\n",
    "and deep learning and the flexible numerical computation core is used across\n",
    "many other scientific domains. This project uses Tensorflow to create a Speech Recognition application with Convolutional Neural Networks.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">\n",
    "TensorFlow is an open-source software library for dataflow\n",
    "programming across a range of tasks. It is a symbolic math library, and is also\n",
    "used for machine learning applications such as neural networks. It is used for\n",
    "both research and production at Google,‍ often replacing its closed-source\n",
    "predecessor, DistBelief.\n",
    "\n",
    "TensorFlow was developed by the Google Brain team for internal Google use.\n",
    "It was released under the Apache 2.0 open source license on November 9,\n",
    "2015.\n",
    "\n",
    "TensorFlow is Google Brain&#39;s second generation system. Version 1.0.0 was\n",
    "released on February 11, 2017. While the reference implementation runs on\n",
    "single devices, TensorFlow can run on multiple CPUs and GPUs (with\n",
    "optional CUDA and SYCL extensions for general-purpose computing on\n",
    "graphics processing units). TensorFlow is available on 64-bit Linux, macOS,\n",
    "Windows, and mobile computing platforms including Android and iOS.\n",
    "\n",
    "Tensorflow was originally created for tasks that require\n",
    "heavy numerical computations and was geared towards the problem of machine learning, and deep neural networks.Due to a C C++ backend, TensorFlow is able to run faster than pure Python code.\n",
    "\n",
    "It provides both a Python and a C++ API. But the Python API is more complete and it's generally easier to use.\n",
    "\n",
    "\n",
    "TensorFlow application uses a structure known as a data flow graph and its structure is based on the execution of this graph. A data flow graph has two basic units.\n",
    "\n",
    "1. A node represents a mathematical operation\n",
    "2. An edge represents a multi-dimensional array, known as a tensor.\n",
    "\n",
    "So this high-level abstraction reveals how the data flows between operations. The standard usage is to build a graph and then execute after the session is created, by using the 'run' and 'eval' operations.\n",
    "\n",
    "\n",
    "The\n",
    "name TensorFlow derives from the operations that such neural networks\n",
    "perform on multidimensional data arrays. \n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Advantages Of Tensorflow:\n",
    "\n",
    "<br/>\n",
    "<div style=\"text-align: justify\">\n",
    "<ol>\n",
    "    <li> <b>Flexibility</b>: we need to express our computation as a data flow graph to use TensorFlow. It is a highly flexible system which provides multiple models or multiple versions of the same model can be served simultaneously. The architecture of TensorFlow is highly modular, which means we can use some parts individually or can use all the parts together. Such flexibility facilitates non-automatic migration to new models/versions, A/B testing experimental models, and canarying new models.</li>\n",
    "\n",
    "<li><b>Portability</b>: TensorFlow has made it possible to play around an idea on our laptop without having any other hardware support. It runs on GPUs, CPUs, desktops, servers, and mobile computing platforms. we can deploy a trained model on our mobile as a part of our product, and that’s how it serves as a true portability feature.</li>\n",
    "\n",
    "<li><b>Research and Production</b>: It can be used to train and serve models in live mode to real customers. To put it simply, rewriting codes is not required and the industrial researchers can apply their ideas to products faster. Also, academic researchers can share codes directly with greater reproducibility. In this way it helps to carry out research and production processes faster.</li>\n",
    "\n",
    "<li> <b>Auto Differentiation</b>: It has automatic differentiation capabilities which benefits gradient based machine learning algorithms. We can define the computational architecture of your predictive model, combine it with our objective function and add data to it- TensorFlow manages derivatives computing processes automatically. We can compute the derivatives of some values with respect to some other values results in graph extension and we can see exactly what’s happening.</li>\n",
    "\n",
    "<li> <b>Performance</b>: TensorFlow allows us to make the most of your available hardware with its advanced support for threads, asynchronous computation, and queues. Just assign compute elements of your TensorFlow graph to different devices and let it manage the copies itself. It also facilitates us with the language options to execute our computational graph. TensorFlow iPython notebook helps in keeping codes, notes, and visualization in a logically grouped and interactive style.</li>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Speech Recognition with Tensorflow\n",
    "<br/>\n",
    "<div style=\"text-align: justify\">\n",
    "Speech recognition is the process of extracting text transcriptions or some form of meaning from speech input. Speech analytics can be considered as the part of the voice processing, which converts human speech into digital forms suitable for storage or transmission computers.\n",
    "\n",
    "This application is capable of detecting the word spoken in the audio file, it uses convolutional neural networks to classify different audio sounds. The convolutional neural networks itself is implemented using tensorflow.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Convoltional Neural Networks\n",
    "<br/>\n",
    "<div style=\"text-align: justify\">\n",
    "The Convolutional neural network(CNN) is a deep learning architecture that has numerous application in computer vision and natural language processing. The CNN classifies objects based on number of features matched.\n",
    "\n",
    "Steps involed in creating CNN\n",
    "    <ol>\n",
    "<li>Convolution</li>\n",
    "<li> Pooling</li>\n",
    "<li> Flattening</li>\n",
    "<li> Full Connection</li>\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ![Dataset](convo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li><b>Convolution</b>\n",
    "<br/>\n",
    "<div style=\"text-align: justify\">\n",
    "ConvNets derive their name from the “convolution” operator. The primary purpose of Convolution in case of a ConvNet is to extract features from the input image. Convolution preserves the spatial relationship between pixels by learning image features using small squares of input data. \n",
    "</div>\n",
    "    </li>\n",
    "<li> <b>Pooling</b>\n",
    "<br/>\n",
    "<div style=\"text-align: justify\">\n",
    "Spatial Pooling (also called subsampling or downsampling) reduces the dimensionality of each feature map but retains the most important information. Spatial Pooling can be of different types: Max, Average, Sum etc.\n",
    "In case of Max Pooling, we define a spatial neighborhood (for example, a 2×2 window) and take the largest element from the rectified feature map within that window. Instead of taking the largest element we could also take the average (Average Pooling) or sum of all elements in that window. In practice, Max Pooling has been shown to work better.\n",
    "</div>\n",
    "    </li>\n",
    "<li> <b>Flattening</b>\n",
    "<br/>\n",
    "<div style=\"text-align: justify\">\n",
    "Convert the 2D matrix to a column vector so that it can passed through an artificial neural network\n",
    "</div>\n",
    "    </li>\n",
    "<li> <b>Full Connection</b>\n",
    "<br/>\n",
    "<div style=\"text-align: justify\">\n",
    "The Fully Connected layer is a traditional Multi Layer Perceptron that uses a softmax activation function in the output layer (other classifiers like SVM can also be used, but will stick to softmax in this post). The term “Fully Connected” implies that every neuron in the previous layer is connected to every neuron on the next layer.\n",
    "The output from the convolutional and pooling layers represent high-level features of the input image. The purpose of the Fully Connected layer is to use these features for classifying the input image into various classes based on the training dataset. \n",
    "</div>\n",
    "    </li>\n",
    "   </ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Implementation of Speech Recognizer in Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to import the libararies required by the application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#import required libraries\n",
    "\n",
    "##for numerical computations\n",
    "import numpy as np  \n",
    "\n",
    "##for converting to mfcc\n",
    "import librosa   \n",
    "\n",
    "##for file handling\n",
    "import os    \n",
    "\n",
    "##for one shot encoding\n",
    "from keras.utils import to_categorical \n",
    "\n",
    "##split dataset into train and test set\n",
    "from sklearn.model_selection import train_test_split  \n",
    "\n",
    "##tensorflow\n",
    "import tensorflow as tf                               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is located in folder named data, we set DATA_PATH variable to that folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"./data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data consists of three classes, viz. bed, cat and happy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"download.png\" alt = \"DataSet\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each folder contains approximately 1700 audio files. The name of the folder is actually the label of those audio files. The task will be to classify an audio between __bed, cat and happy__ - these three classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Preprocessing Audio Files "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to prepare a fixed size vector for each audio file and feed the vector into the Convolutional Net.An __embedding__ is a mapping from discrete objects, such as words, to vectors of real numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Audio Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Audion Embedding](Aembed.png)\n",
    "\n",
    "In sound processing, the __mel-frequency cepstrum__ (MFC) is a representation of the short-term power spectrum of a sound, based on a linear cosine transform of a log power spectrum on a nonlinear mel scale of frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Steps\n",
    "\n",
    "1. Read the audio file from DATA_PATH.\n",
    "\n",
    "2. Perform downsample operation.\n",
    "\n",
    "3. Compute MFCC using librosa library.\n",
    "\n",
    "4. MFCC vectors might vary in size for different audio input,To overcome this problem we need to pad the output vectors with zero.\n",
    "\n",
    "The function __wav2mfcc__ performs the aforementioned steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wav2mfcc(file_path, max_pad_len=11):\n",
    "    wave, sr = librosa.load(file_path, mono=True, sr=None)\n",
    "    wave = wave[::3]\n",
    "    mfcc = librosa.feature.mfcc(wave, sr=16000)\n",
    "    pad_width = max_pad_len - mfcc.shape[1]\n",
    "    mfcc = np.pad(mfcc, pad_width=((0, 0), \n",
    "                    (0, pad_width)), mode='constant')\n",
    "    return mfcc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function __get_labels__ gets the name of different categories of data present in the dataset folder and encodes them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(path=DATA_PATH):\n",
    "    labels = os.listdir(path)\n",
    "    label_indices = np.arange(0, len(labels))\n",
    "    return labels, label_indices, to_categorical(label_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to avoid the computation of mfcc again and again we store the calculated data in a numpy array using the __save_data_to_array__ function and can be resued for further computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data_to_array(path=DATA_PATH, max_pad_len=11):\n",
    "    labels, _, _ = get_labels(path)\n",
    "\n",
    "    for label in labels:\n",
    "        # Init mfcc vectors\n",
    "        mfcc_vectors = []\n",
    "\n",
    "        wavfiles = [path + label + '/' + wavfile \n",
    "                    for wavfile in os.listdir(path + '/' + label)]\n",
    "        for wavfile in wavfiles:\n",
    "            mfcc = wav2mfcc(wavfile, max_pad_len=max_pad_len)\n",
    "            mfcc_vectors.append(mfcc)\n",
    "        np.save(label + '.npy', mfcc_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function __get_train_test__ splits the entire dataset into training set and test set. The training set contains 60 percent of the data and test set contains 40 percent of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test(split_ratio=0.6, random_state=42):\n",
    "    # Get available labels\n",
    "    labels, indices, _ = get_labels(DATA_PATH)\n",
    "\n",
    "    # Getting first arrays\n",
    "    X = np.load(labels[0] + '.npy')\n",
    "    y = np.zeros(X.shape[0])\n",
    "\n",
    "    # Append all of the dataset into one single array, same goes for y\n",
    "    for i, label in enumerate(labels[1:]):\n",
    "        x = np.load(label + '.npy')\n",
    "        X = np.vstack((X, x))\n",
    "        y = np.append(y, np.full(x.shape[0], fill_value= (i + 1)))\n",
    "\n",
    "    assert X.shape[0] == len(y)\n",
    "\n",
    "    return train_test_split(X, y,\n",
    "                            test_size= (1 - split_ratio),\n",
    "                            random_state=random_state, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use get_train_test to split the dataset, \n",
    "We also use one hot encoding to convert categorical data\n",
    "to appropriate formats which is suitable for computation \n",
    "by the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = get_train_test()\n",
    "X_train = X_train.reshape(X_train.shape[0], 20, 11, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 20, 11, 1)\n",
    "y_train_hot = to_categorical(y_train)\n",
    "y_test_hot = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Building the Convolutional Neural Network\n",
    "\n",
    "The function __init_weights__ initializes the weights of the network with some random values using tensorflow's random distribution function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing the weights of the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(shape):\n",
    "    init_random_dist = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(init_random_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing the bias of the CNN\n",
    "\n",
    "This function __init_bias__ initializes the bias with some constant value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_bias(shape):\n",
    "    init_bias_vals = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(init_bias_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the Convolutional layer\n",
    "The __conv2d__ is a function that takes feature matrix x, and weight matrix W as input \n",
    "and returns a 2d convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1],\n",
    "                        padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 2x2 pooling layer is created by __max_pool_2by2__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_pool_2by2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__convolutional_layer__ method uses conv2d to perform convolution and passes it\n",
    "through a relu activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolutional_layer(input_x, shape):\n",
    "    W = init_weights(shape)\n",
    "    b = init_bias([shape[3]])\n",
    "    return tf.nn.relu(conv2d(input_x, W) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__normal_full_layer__ creates a fully connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_full_layer(input_layer, size):\n",
    "    input_size = int(input_layer.get_shape()[1])\n",
    "    W = init_weights([input_size, size])\n",
    "    b = init_bias([size])\n",
    "    return tf.matmul(input_layer, W) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual CNN is then created with the help of above helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-1cee26eb7243>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m## And there will be 32 output features,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m## hence shape = [2, 2, 1, 32]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mconvo_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvolutional_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m## flattening the features extracted into an 1D array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "## using a 2x2 feature detector for convolution\n",
    "## There is a single input channel \n",
    "## And there will be 32 output features,\n",
    "## hence shape = [2, 2, 1, 32]\n",
    "convo_1 = convolutional_layer(x,shape=[2,2,1,32])\n",
    "\n",
    "## flattening the features extracted into an 1D array\n",
    "## The size of the array will be 20x11x32\n",
    "convo_2_flat = tf.reshape(convo_1,[-1,20*11*32])\n",
    "\n",
    "## creating a fully connected laer with 1024 neurons\n",
    "## It takes the 1D array conv_2_flat as input\n",
    "full_layer_one = tf.nn.relu(normal_full_layer(convo_2_flat,1024))\n",
    "\n",
    "## during training phase we need to randomly turn off \n",
    "## some neurons to find new paths in the network and \n",
    "## increase accuracy\n",
    "full_one_dropout = tf.nn.dropout(full_layer_one,keep_prob=hold_prob)\n",
    "\n",
    "## creating the final output layer with 3 neurons\n",
    "## to classify the audio into 3 categories\n",
    "y_pred = normal_full_layer(full_one_dropout,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the cost function of the network. In this case cross entropy is being used.\n",
    "The objective of the CNN is to optimize the cost function. We use Adam optimizer for\n",
    "our application. The learning rate $\\alpha$ is 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_true,\n",
    "                                               logits=y_pred))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "train = optimizer.minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Training the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow uses placeholders as special variables to feed data into Deep Neural Networks.\n",
    "We define three placeholders as described below-\n",
    "1. x: input\n",
    "2. y_true: correct labels\n",
    "3. hold_prob: probability for dropout layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32,shape=[None,20,11,1])\n",
    "y_true = tf.placeholder(tf.float32,shape=[None,3])\n",
    "hold_prob = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph nodes corresponding to the CNN are created by Tensorflow, \n",
    "and the variables are initialized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__saver__ saves the model parameters and weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a Tensorflow session. The graph is initialized and the session is run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(250):\n",
    "        #print(\"Training Epoch : \" + str(i))\n",
    "        sess.run(train, feed_dict={x: X_train,\n",
    "                                   y_true: y_train_hot, hold_prob: 0.5})\n",
    "        \n",
    "        # PRINT OUT A MESSAGE EVERY 100 STEPS\n",
    "        if i%100 == 0:\n",
    "            \n",
    "            print('Currently on step {}'.format(i))\n",
    "            print('Accuracy is:')\n",
    "            # Test the Train Model\n",
    "            matches = tf.equal(tf.argmax(y_pred,1),tf.argmax(y_true,1))\n",
    "\n",
    "            acc = tf.reduce_mean(tf.cast(matches,tf.float32))\n",
    "            print(sess.run(acc,feed_dict={x:X_test,\n",
    "                                          y_true:y_test_hot,hold_prob:1.0}))\n",
    "    \n",
    "    saver.save(sess,'models/cnn_model.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Creating a GUI and predicting a new audio clip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing libraries for creating GUI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tkinter import filedialog\n",
    "from tkinter import Tk,Label,Button,Canvas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"justify\">\n",
    "    We now create a browse button for browsing and loading audio clips.\n",
    "After loading the clip, we calculate the mfcc of the audio.\n",
    "A Tensorflow session is created.\n",
    "The trained CNN is loaded and the saved parameters are restored.\n",
    "Prediction is made on the audio clip which is then output and \n",
    "the output is displayed on canvas.\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def browse_button():\n",
    "    # Allow user to select a directory and store it in global var\n",
    "    # called folder_path\n",
    "    global folder_path\n",
    "    global path\n",
    "    filename = filedialog.askopenfile()\n",
    "    sample = wav2mfcc(filename.name)\n",
    "    print(filename.name)\n",
    "    path = \"aplay \" + filename.name[50:]\n",
    "    sample_reshaped = sample.reshape(1, 20, 11, 1)\n",
    "    labels = [\"happy\", \"cat\", \"bed\"]\n",
    "    with tf.Session() as sess :\n",
    "        saver.restore(sess,'models/cnn_model.ckpt')\n",
    "        predict = tf.argmax(y_pred,1)\n",
    "        pred = sess.run(predict,\n",
    "                        feed_dict={x:sample_reshaped,\n",
    "                                   y_true: y_train_hot, hold_prob: 1.0})\n",
    "        ans = labels[pred[0]]\n",
    "        canvas.create_text(350, 25, \n",
    "                text = \"The detected word is : \" + ans, font=(\"Purisa\", 25)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main code which creates the gui with the help of browse button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = Tk()\n",
    "root.title(\"Welcome to voice predictor\") \n",
    "root.geometry('800x600')\n",
    "button2 = Button(text=\"Browse Audio File\",\n",
    "                 command=browse_button, height = 1,\n",
    "                 width = 20,font=(\"Ariel\", 15))\n",
    "button2.pack()\n",
    "canvas = Canvas(width=700, height=50, bg='green')\n",
    "canvas.pack()\n",
    "play = lambda : os.system(path)\n",
    "button = Button(root, text = 'Play Word Sound',\n",
    "                command = play, height = 1, width = 20,font=(\"Purisa\", 15))\n",
    "button.pack()\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 A sample run "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>The input window</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"Gui1.png\" height= 30% width = 40%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> Predicting Output </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"gui2.png\" width = 40% />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
